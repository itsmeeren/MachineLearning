Classification -yes no true false
Regression - continous values needs to be predicted
clustering - Data need to be arranged to produce output
Dimensional reduction





**Linear regression**
	Continuous values

	Numerical  - number based diffrentiation
	categorical - charecteristics based diffrentiation
	
	
	supervised
		Regression -simple linear,multiple linear,polynomial linear 
		classification
		
	[from scipy.stats import linregress]-> for regression finding
	best fit line has the least square distance between the datapoints
	
	Multiple linear regression

	onehot encoder is used for converting coatagorical data to binary vectors like 'red'=[1 1 0]
	
	
	
**Logisticis regression**
	gives discrete values
	This is used for classification purpose.
	Dataset with one or more dependent variable used to determine binary output of dependent variable
	
	Classification algorithms
		->Descision Tree - each branch node represents a choice leafnode represents decision
		-> K_Nearrest Neighbour - determine the given object is ,based on similarity to the object
		it is compared to
	odds=probrability of happening/probability of not happening
	
	o=p/1-p
	equation of the sigmoid function =p(x)= 1/1+e^-(b0+b1x)
	
**Descision Tree**
	
	used for feature (categorical) based diffrentiation
	
	Disadvantages
	
	overfitting occurs when the algorithm captures the noise in the data	
	High variance occurs model become unstable for the small change in the data
	Low biased tree -highly complicated descision tree tends to have low bias data which makes 
	difficult for the model to work with new data
	
	Steps
	
	Entropy is the randomness in the dataset
	Information gain -measure of decrease in entropy after the dataset is split
	
	leafnode -carries descision
	descisionnode -have two or more branches
	rootnode-top of descision node
	
	entropy= sum p_i*log(p_i)    from i =1 to k where i is the possible classes
	
	shape of the data is (x,y) x=no of rows
				   y=no of columns
	
	
**Random Forest**

	accuracy is higher and training time is less
	multiclass object detection is done using this algorithm provides better detection complicated
	environments
	used in kinect -game console for body movement detection
	
	
	
	Method that operates by creating multiple trees during the training phase 
	The descision of the majority of the trees is chosen by random forest as the final descision
	descision tree is the tree shaped diagram used to determine the course of action,each branch of the
	tree represents the possible descision,occurence or reaction


**Naive Bayes**

	It woerks on the principle of conditional probability as given by the BAYES' theorem
	P(A | B) = ( P(B | A) * P(A) ) / P(B)
	it calculates the conditional probability based on the prior knowledge of conditions that might be
	happening related to the event
	
	Uses:
		Face recognition , Weather prediction 
	Advantages
		
		Not sensitive to irrelavent features 
		used in real time application because its fast
		highly scalable with number of predictors and data points
		needs less training data
		used for both discrete and continuos data
		
**Support Vector Machine**
	
	svm is supervised learning method that looks at data and sorts it into one of the two categories
	Distance between the support vectors and hyperplane should be far as possible
	Hyper plane has the maximunm distance to the support vectors of any class
	two terms D+ and D-
	
	D+ is the shortest distance to the closest positive points
	D- is the shortest distance to the closest negetive points
	Sum of D+ and D- is called the distance margin
	after finding the largest distance point we can get the optimal hyperplane
	
	Working
	
	If the datas interfere within a class of one another then 'Kernel' function is used to transform 
	them into 1D to 2D
	suppose if the datas inside the 2D interferes then it should he converted to 3D for creating the
	Hyperplane
	
	
	Advantages
		
		High dimensional input space 
		Sparse document vectors
		Regularization parameter
	 
	
**K-Nearest neighbour(KNN)**
	
	This classifies the datapoint based on how its neighbors are clasified
	KNN stores all the available cases and classifies new classes based on similartiy measure
	Data points are classfied  by the majority of the votes from its nearest neighbors
	
	K is likme circle which is used to diffrentiate between the data points and object is compared 
	with the other objects and classified according to that 
	
	finding the right value of k is known as paramter tuning 
	right value of k is important for the accuracy
	
	selecting the value of k
		
		sqrt(n) where n is the number of datapoints
		odd value of k is choosen to avoid confusion between two classes of data
	
	used when 
		
		when the data is labeled
		data is noise free
image	
	working 
	
	
		to find the nearest neighbor in a plane Euclidian distance formula is used
		after finding distance we look at nearest distances then we take k value 
	In code
	
	standard scaler -preprocessor for avoiding the Bias

**convulational Nueral Network**
	
	
	Image Recognition
		
		it takes the input as the pixels of image in the form of array
		hidden layers carry out the feature extraction and do some calculation and 
		manipulation
		there are multiple layers 
			*Relu layer
			*convulution layer
			*pooling layer -pooling information together
		convulaution layer uses matrix filter and performs the convulution operation to
		detect the patterns in the image
		
	flattening layer
		flattening is the process of converting all resultant 2 dimensional array from pooled
		map into a single continuous linear vector
		
		then it is fed into the fully connected layer as the input to classify the image
	
		
	
	
	Relu function - 
		
		*performs elementwise operstion
		*sets all the negetive pixels to 0
		*introduces non linearity to the model
		*then output is the rectified feature map
	
	
	pooling 
		
		it is the down sampling operation which reduces the dimensionality of the feature 
		map

	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
